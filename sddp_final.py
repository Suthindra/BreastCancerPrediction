# -*- coding: utf-8 -*-
"""SDDP Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QuLZfnc-Jm9zFTtuiigEk9eSiwd5DZsb

# Imports
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
# %matplotlib inline

"""# Google Drive Mount & Setup"""

from google.colab import drive
drive.mount('/content/drive')

# reading data into the dataframe
df = pd.read_csv('/content/drive/MyDrive/SDDP Dataset/dataset.csv')

"""# Basic Operations on the Dataset"""

# displaying first few rows
df.head()

# shape of the dataframe
df.shape

# concise summary of dataframe
df.info()

# column names
df.columns

# checking for null values
df.isnull().sum()

# dropping 'Unnamed: 32' column.
df.drop("Unnamed: 32", axis=1, inplace=True)

# dropping id column
df.drop('id',axis=1, inplace=True)

# descriptive statistics of data
df.describe()

"""# Count Plot"""

# countplot
plt.figure(figsize = (8,7))
sns.countplot(x="diagnosis", data=df, palette='magma')

# Mapping categorical values to numerical values
df['diagnosis'] = df['diagnosis'].map({'B': 0, 'M': 1})

"""# Splitting Data into Train & Test"""

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    df.drop('diagnosis', axis=1),
    df['diagnosis'],
    test_size=0.2,
    random_state=42
)

# Standardizing the data
ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.transform(X_test)

"""# Data Pre-processing"""

# counts of unique rows in the 'diagnosis' column
df['diagnosis'].value_counts()

from sklearn.model_selection import train_test_split

# splitting data
X_train, X_test, y_train, y_test = train_test_split(
                df.drop('diagnosis', axis=1),
                df['diagnosis'],
                test_size=0.2,
                random_state=42)

print("Shape of training set:", X_train.shape)
print("Shape of test set:", X_test.shape)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.fit_transform(X_test)

"""# Data Visualization"""

# heatmap
plt.figure(figsize=(20,18))
sns.heatmap(df.corr(), annot=True,linewidths=.5, cmap="Purples")

df.columns

# Getting Mean Columns with diagnosis
m_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',
       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

# Getting Se Columns with diagnosis
s_col = ['diagnosis','radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
       'fractal_dimension_se']

# Getting Worst column with diagnosis
w_col = ['diagnosis','radius_worst', 'texture_worst',
       'perimeter_worst', 'area_worst', 'smoothness_worst',
       'compactness_worst', 'concavity_worst', 'concave points_worst',
       'symmetry_worst', 'fractal_dimension_worst']

# pairplot for mean columns
sns.pairplot(df[m_col],hue = 'diagnosis', palette='Blues')

# pairplot for se columns
sns.pairplot(df[s_col],hue = 'diagnosis', palette='Greens')

# pairplot for worst columns
sns.pairplot(df[w_col],hue = 'diagnosis', palette='Oranges')

"""# Defining Models & COnfusion Matrices"""

# Classification Models

# 1. Random Forest
rfc = RandomForestClassifier(n_estimators=300, random_state=42)
rfc.fit(X_train, y_train)
rfc_predictions = rfc.predict(X_test)

print("Random Forest Confusion Matrix: \n", confusion_matrix(y_test, rfc_predictions))
print("\n")
print(classification_report(y_test, rfc_predictions))

rfc_acc = accuracy_score(y_test, rfc_predictions)
print("Accuracy of Random Forests Model is: ", rfc_acc)

# 2. Gradient Boosting for Optimization

gbc = GradientBoostingClassifier(n_estimators=300, random_state=42)
gbc.fit(X_train, y_train)
gbc_predictions = gbc.predict(X_test)

print("Gradient Boosting Confusion Matrix: \n", confusion_matrix(y_test, gbc_predictions))
print("\n")
print(classification_report(y_test, gbc_predictions))

gbc_acc = accuracy_score(y_test, gbc_predictions)
print("Accuracy of Gradient Boosting Model is: ", gbc_acc)

# 3. Voting Classifier (using Random Forest and Gradient Boosting)

voting_clf = VotingClassifier(estimators=[('rfc', rfc), ('gbc', gbc)], voting='hard')
voting_clf.fit(X_train, y_train)
voting_predictions = voting_clf.predict(X_test)

print("Voting Classifier Confusion Matrix: \n", confusion_matrix(y_test, voting_predictions))
print("\n")
print(classification_report(y_test, voting_predictions))

voting_acc = accuracy_score(y_test, voting_predictions)
print("Accuracy of Voting Classifier Model is: ", voting_acc)

"""# Accuracy Finding & PLotting for Models"""

# Final Results

print(f"Random Forest Accuracy: {rfc_acc}")
print(f"Gradient Boosting Accuracy: {gbc_acc}")
print(f"Voting Classifier Accuracy: {voting_acc}")

# Plotting the accuracy of all models
plt.figure(figsize=(12, 6))
model_acc = [rfc_acc, gbc_acc, voting_acc]
model_name = ['Random Forests', 'Gradient Boosting', 'Voting Classifier']
sns.barplot(x=model_acc, y=model_name, palette='magma')
plt.xlabel('Accuracy')
plt.title('Model Accuracy Comparison')
plt.show()